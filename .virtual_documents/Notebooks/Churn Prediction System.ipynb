import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import accuracy_score, precision_score, recall_score, classification_report
import warnings
warnings.filterwarnings("ignore")


df = pd.read_csv("../Data/Telco-Customer-Churn.csv")
df


df.isnull().sum()


df.info()


df.describe()


# Remove extra spaces from column names
df.columns = df.columns.str.strip()



# Drop customerID (not useful for prediction)
df.drop('customerID', axis=1, inplace=True, errors='ignore')
df


# Convert TotalCharges from object to numeric
df['TotalCharges'] = pd.to_numeric(df['TotalCharges'], errors='coerce')


# Fill missing TotalCharges with median
df['TotalCharges'] = df['TotalCharges'].fillna(df['TotalCharges'].median())



# Convert target variable to binary

df['Churn'] = df['Churn'].replace({'Yes': 1, 'No': 0})


from sklearn.preprocessing import LabelEncoder, StandardScaler

# Identify categorical columns
cat_cols = df.select_dtypes(include='object').columns

# Apply Label Encoding
le = LabelEncoder()
for col in cat_cols:
    df[col] = le.fit_transform(df[col])



X = df.drop('Churn', axis=1)
y = df['Churn']


from sklearn.model_selection import train_test_split

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)


from sklearn.preprocessing import LabelEncoder, StandardScaler

scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)



from sklearn.linear_model import LogisticRegression

log_reg = LogisticRegression(max_iter=1000)
log_reg.fit(X_train_scaled, y_train)


y_pred_lr = log_reg.predict(X_test_scaled)

print("Logistic Regression")
print("Accuracy:", accuracy_score(y_test, y_pred_lr))
print("Precision:", precision_score(y_test, y_pred_lr))
print("Recall:", recall_score(y_test, y_pred_lr))
print(classification_report(y_test, y_pred_lr))




rf = RandomForestClassifier(
    n_estimators=200,
    max_depth=10,
    random_state=42
)


rf.fit(X_train, y_train)


y_pred_rf = rf.predict(X_test)
y_prob_rf = rf.predict_proba(X_test)[:, 1]



print("Random Forest")
print("Accuracy:", accuracy_score(y_test, y_pred_rf))
print("Precision:", precision_score(y_test, y_pred_rf))
print("Recall:", recall_score(y_test, y_pred_rf))
print(classification_report(y_test, y_pred_rf))



!pip install xgboost


from xgboost import XGBClassifier

xgb = XGBClassifier(
    n_estimators=200,
    learning_rate=0.05,
    max_depth=5,
    random_satate=42,
    eval_metric='logloss'
)


xgb.fit(X_train, y_train)



y_pred_xgb = xgb.predict(X_test)
y_prob_xgb = xgb.predict_proba(X_test)[:, 1]



rf.fit(X_train, y_train)


importances = rf.feature_importances_
features = X.columns

feature_importance_df = pd.DataFrame({
    'Feature': features,
    'Importance': importances
}).sort_values(by='Importance', ascending=False)

feature_importance_df.head(10)


churn_prob_df = X_test.copy()
churn_prob_df['Actual_Churn'] = y_test.values
churn_prob_df['Churn_Probability'] = y_prob_xgb

churn_prob_df.head()


churn_prob_df['Risk_Level'] = churn_prob_df['Churn_Probability'].apply(
    lambda x: 'High' if x > 0.7 else 'Medium' if x > 0.4 else 'Low'
)


plt.hist(churn_prob_df['Churn_Probability'], bins=20)
plt.xlabel("Churn Probability")
plt.ylabel("Number of Customers")
plt.title("Customer Churn Risk Distribution")
plt.show()
